# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eNovRdh4ThvG0NohZnWIzKzx8o-afKsR
"""

# ==== Install Libraries ====
!pip install praw requests pandas transformers wordcloud matplotlib

import praw
import requests
import pandas as pd
import re
from transformers import pipeline
from wordcloud import WordCloud
import matplotlib.pyplot as plt

REDDIT_CLIENT_ID = "XdmrvmxBpZkQrMlvmk9gaw"        # <-- Replace with your Reddit client_id
REDDIT_CLIENT_SECRET = "6bnOp66d1wmBtWlE93qZwrV6NLIM8Q" # <-- Replace with your Reddit client_secret
USER_AGENT = "cursor_feedback_analysis"

# ==== Collect Reddit Posts ====
reddit = praw.Reddit(
    client_id=REDDIT_CLIENT_ID,
    client_secret=REDDIT_CLIENT_SECRET,
    user_agent=USER_AGENT
)

reddit_posts = []
for post in reddit.subreddit("all").search("Cursor IDE", limit=100):
    post.comments.replace_more(limit=0)
    comments = " ".join([comment.body for comment in post.comments.list()])
    reddit_posts.append({
        "source": "Reddit",
        "title": post.title,
        "text": post.selftext,
        "score": post.score,
        "comments": comments,
        "link": f"https://www.reddit.com{post.permalink}"
    })

print(f"Reddit posts collected: {len(reddit_posts)}")

# ==== Collect Stack Overflow Posts ====
url = "https://api.stackexchange.com/2.3/search"
params = {
    "order": "desc",
    "sort": "activity",
    "intitle": "Cursor IDE",
    "site": "stackoverflow"
}
response = requests.get(url, params=params)
data = response.json()

stack_posts = []
for item in data.get("items", []):
    stack_posts.append({
        "source": "Stack Overflow",
        "title": item.get("title", ""),
        "text": "",
        "score": item.get("score", 0),
        "comments": "",
        "link": item.get("link", "")
    })

print(f"Stack Overflow posts collected: {len(stack_posts)}")

# ==== Merge both sources ====
all_posts = reddit_posts + stack_posts
df_all = pd.DataFrame(all_posts)

# ==== Clean text ====
def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"[^a-z0-9\s]", "", text)
    return text

df_all["content_clean"] = df_all.apply(
    lambda row: clean_text((row.get("title","") + " " + row.get("text","") + " " + row.get("comments",""))),
    axis=1
)
print("Data merged and cleaned!")
df_all.head()

# ==== Sentiment Analysis ====
sentiment_classifier = pipeline("sentiment-analysis")
df_all["sentiment"] = df_all["content_clean"].apply(lambda x: sentiment_classifier(x[:512])[0]['label'])
print("Sentiment analysis completed!")
df_all.head()

# ==== Zero-Shot Classification ====
topic_classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

# Candidate topics
candidate_labels = ["bug report", "feature request", "praise", "performance issue", "usability feedback"]

def classify_topic(text):
    result = topic_classifier(text[:512], candidate_labels)
    return result['labels'][0]

df_all["topic"] = df_all["content_clean"].apply(classify_topic)
print("Topic classification completed!")
df_all.head()

# ==== Sentiment Distribution Chart ====
sentiment_counts = df_all["sentiment"].value_counts()
sentiment_counts.plot(kind="bar", title="Sentiment Distribution", xlabel="Sentiment", ylabel="Count")
plt.show()

# ==== Topic Distribution Chart ====
topic_counts = df_all["topic"].value_counts()
topic_counts.plot(kind="bar", title="Topic Distribution", xlabel="Topic", ylabel="Count")
plt.show()

# ==== Word Cloud ====
words = " ".join(df_all["content_clean"]).split()
wordcloud = WordCloud(width=800, height=400).generate(" ".join(words))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

# ==== Summarization ====
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Reddit Summary
reddit_text = " ".join(df_all[df_all["source"]=="Reddit"]["content_clean"].tolist())[:3000]
reddit_summary = summarizer(reddit_text, max_length=130, min_length=50, do_sample=False)[0]['summary_text']

# Stack Overflow Summary
so_text = " ".join(df_all[df_all["source"]=="Stack Overflow"]["content_clean"].tolist())[:3000]
so_summary = summarizer(so_text, max_length=130, min_length=50, do_sample=False)[0]['summary_text']

print("=== REDDIT SUMMARY ===")
print(reddit_summary)
print("\n=== STACK OVERFLOW SUMMARY ===")
print(so_summary)

# ==== Save final CSV ====
df_all.to_csv("cursor_feedback_final.csv", index=False)

# ==== Download file ====
from google.colab import files
files.download("cursor_feedback_final.csv")

print("Final file with sentiment, topic classification, and cleaned text saved as cursor_feedback_final.csv")